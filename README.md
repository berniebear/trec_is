# TREC-IS
TREC Incident Stream shared task (official website [here](http://dcs.gla.ac.uk/~richardm/TREC_IS/))

## Environment

`Python 3.6`

TensorFlow, scikit-learn==0.19.2

As we don't have sudo on server, we use conda to manage all packages.

### How to Run
```bash
python3 main.py
```

## Task Description

### [Important] Change in 2019 compared with 2018
[Here](http://dcs.gla.ac.uk/~richardm/TREC_IS/2019/2019Changes.html) is a page to describe the differents.

### Event Types
wildfire, earthquake, flood, typhoon/hurricane, bombing, shooting

### Task
There is only a single task for the first year of the track (2018): classifying tweets by information type (high-level).

The goal of this task is for systems to categorize the tweets in each event/incident's stream into different information feeds.

the task aim is to assign ontology labels (information types) to each tweet within the event stream.

As noted above, the ontology has multiple layers, moving from generic information types to the
very specific. For this reason, we denote information types as either `top-level intent’, ‘high-level’
or ‘low-level’. For example, a top-level intent might be ‘Reporting’ (the user is reporting some
information). Within reporting, a high-level type might be ‘Service Available’ (the user is
reporting that some service is being provided). Within service available, a low-level type might
be ‘Shelter Offered’ (shelter is offered for affected citizens).

This task is Classifying Tweets by Information Type (high-level). I.e. the goal is to categorize
tweets into the information types listed as high-level. One category per tweet.

### Submission format
The submission format is slightly different from the TREC-eval format, and you can find the requirement [here](http://dcs.gla.ac.uk/~richardm/TREC_IS/TREC_2018_Incident_Streams_Guidelines.pdf)

## Data

`ITR-H.types.v2.json` Contains the high-level label which we need to predict for tweets.

`TRECIS-CTIT-H-Training.json` Contains the training data
  - `postID` is the tweet id and we can use it to get tweet text and img
  - `categories` is the category we want to classify
  - `priority` will be mapped to score (will it incluence the evaluation?)
  - `indicatorTerms` are some important terms extracted by the annotator

`TRECIS-CTIT-H-Test.topics` contains 15 topics (events) for test data

`TRECIS-CTIT-H-Test.tweetids.tsv` Contains the Tweets stream for each topic in `TRECIS-CTIT-H-Test.topics`

`TRECIS-2018-TestEvents-Labels` This folder contains the labels of those test events

Tweet ids and json contents (generated by `extract_train_ids_to_file` in `utils.py`)
  - `train-ids.txt` Contains tweets identifiers for training data
  - `test-ids.txt` Contains tweets identifiers for test data
  - `train-tweets.txt` Contains tweets contents for training, each line is a json format
  - `test-tweets.txt` Contains tweets contents for test, each line is a json format

### Statistics Info
25 label categories

Train: 1,335 instances, including 6 events

Test: 22,216 instances, including 15 events

Notice that some tweets are not accessible due to the suspend of the account
- There are 96 such tweets in train, and 1263 in test

## Get Twitter according to IDs
These datasets will be distributed as a list of tweet identifiers for each incident. Participants will
need to fetch the actual JSON tweets using publicly available tools, such as twarc
(https://github.com/DocNow/twarc), the TREC Microblog Track twitter-tools
(https://github.com/lintool/twitter-tools), or any other tool for crawling twitter data.

For twarc tools, you can use this command to get tweets from a file contains ids
```bash
twarc hydrate ids.txt > tweets.jsonl
```

Actually, we can use `twitter.com/user/status/<TWEETID>` and then it will redirect to the true page.
But it requires too many effort to get the json (need to use scrapy and extract json from a dynamic webpage).

To make it easier, we need a developer account on `https://developer.twitter.com/en.html`

Finally, I get Twitter developer account by my Andrew Email, the passwd is generated by Chrome.

The credentials for JunpeiZhou have been saved to your configuration file at /Users/jpzhou/.twarc

```bash
Consumer API keys
yy0XFA7LsdBWiJ0NMLl92JVdp (API key)
bBtK6rk2wf4VTejobPVtARbn5IKPtMPfl02pVSx9lDCgty6nYJ (API secret key)

Access token & access token secret
1096268271286931459-l5CjMwD2VhaliVt811oL80BBXJfzSm (Access token)
IeR7tyuzfyubrZUlnAT5sJL0eLX3o8BljMPO53YoPwv5H (Access token secret)
Read and write (Access level)
```

## Current Result
Baseline
```
Information Type Precision (any valid type, micro): 0.3874517740813358
Information Type Recall (any valid type, micro): 0.6394572025052192
Information Type F1 (any valid type, micro): 0.48253318104840304
Information Type Accuracy (any valid type, micro): 0.335877476748888
```

Add fasttext (avg)
```
Information Type Precision (any valid type, micro): 0.4057043435033866
Information Type Recall (any valid type, micro): 0.6691606026442554
Information Type F1 (any valid type, micro): 0.5051450676982592
Information Type Accuracy (any valid type, micro): 0.3534169025475131
```

Add BERT
```
Information Type Precision (any valid type, micro): 0.4143519950510362
Information Type Recall (any valid type, micro): 0.6791725816264449
Information Type F1 (any valid type, micro): 0.5146962769431743
Information Type Accuracy (any valid type, micro): 0.3615547917509098
```

Add skip-thought
```
Information Type Precision (any valid type, micro): 0.42567274976801733
Information Type Recall (any valid type, micro): 0.6886509207365893
Information Type F1 (any valid type, micro): 0.5261306724777306
Information Type Accuracy (any valid type, micro): 0.3734836231298019
```

Change to data got by jar (less tweets missing)
```
Information Type Precision (any valid type, micro): 0.43939964832956546
Information Type Recall (any valid type, micro): 0.681371116953939
Information Type F1 (any valid type, micro): 0.5342648799297522
Information Type Accuracy (any valid type, micro): 0.38339061868176305
```

Currently rank 8/40 in 2018 leaderboard 


**Experiments to see if cross-val is comparable with previous setting**

2018-train + test perform Cross-validation F1: 0.56, Accuracy: 0.39 (so it is comparable). Here are details
```
2019-04-02 22:53:14,006 - root - INFO - The acc score in cross validation is [0.4090368608799049, 0.38002378121284186, 0.38882282996432815, 0.38106565176022833, 0.3946241674595623]
2019-04-02 22:53:14,006 - root - INFO - The f1 score in cross validation is [0.5778834720570749, 0.5491959190731454, 0.5579635362917097, 0.5509147393855712, 0.5642869371682931]
2019-04-02 22:53:14,006 - root - INFO - The average acc score is 0.3907146582553731
2019-04-02 22:53:14,006 - root - INFO - The average f1 score is 0.5600489207951589
```
2018-train and then test on cross-validation (however, it has a kind of leak, because the train is included in the cross-validation test data).
The average acc score is 0.40 and the average f1 score is 0.55

When use late fusion, the f1 score will drop a lot.

## Todo
- Change labels in data file according to the [changes in 2019](http://dcs.gla.ac.uk/~richardm/TREC_IS/2019/2019Changes.html) 
- Multitask: use the hashtag as the label and try to predict it.
- Read official fasttext ipython notebook to figure out why it performs so high (I guess it dues to parameter search)
- First clasify the higher level, and then classify the target (Request-GoodsService, Request-SearchAndRescue)
- Hand-crafted rule feature for each type
- Add feature for each event (the type/title/narr tag in the event)
- Use a better method to predict the importance score
- Use generative model, try to model the joint distribution of p(x,y) and can extract the feature to feed into the descriptive model (similar to ELMo)
- Borrow idea from Twitter Sentiment Analysis (including pre-processing methods)
- Suggested by Xin: augment interest profile with calling Google API. By the way, remember to deduplicate the tweets (removing tweets of retweet and very similar tweets).
- Relationship between events (domain adaption/shift)
- Use time to predict the priority of tweets, and give high priority tweets more weights in train
- Use indicator terms (such as forming a bag of words) in 2018 training data, because it is a quite important label provided by human labelers

### Mar29 Discussion

Junpei:
- [done] Cross-validation to see if it is comparable (question: Current evaluation is any-type, which means only require to have overlap with the test labels, how to perform it in cross-validation)
- [done] To see if the additional test data added into training is helpful (if data is noisy)
- [done] Late fusion (train model and then average, because different features may not in the same scale; However, for graph-based model such as NB the scale will not influence it)
- [done] Check with host if additional data could be used
- A classifier to classify event (Event-wise model)

Xinyu:
- News extract feature
- Explore the data to see if the number of images is large enough and useful
- Manually check if data timestep contains useful information
- Natural Disaster Dataset. As shown in the TREC-IS official website, there are six general events: bombing, earthquake, flood, hurricane, wildfire, shooting. 
The description of each type of general event could be found in its corresponding pdf ("Event Type Profiles" in [webpage](http://dcs.gla.ac.uk/~richardm/TREC_IS/)). 

Bernie:
- [done] Get computatioinal resource for this project
- Check the correctness of get_clean_tweet in utils
- Classifier part

## Reference

Official Baseline system: https://github.com/cbuntain/trecis

It is quite useful as it includes many features and pre-trained vectors and introduce another large dataset that could be used

Some addtional tweets data: 
- https://crisislex.org/data-collections.html#CrisisLexT26
- https://crisisnlp.qcri.org/
- Get tweets by hashtags and keywords through Twitter API (like the way to collect retrospective data)
